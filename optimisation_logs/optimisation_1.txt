Loading tokenizer from: data/tokenizer.json
Reading text from: data/brown_corpus.txt
Profiling 1000 lines...
--- Profiling Results (Average per line) ---
Normalization:         1567.83 ns
Pre-tokenization:     14406.18 ns
Model Tokenization:    4453.86 ns
Post-processing:        702.69 ns
Total (Pipeline):     21130.55 ns
Total (End-to-End):   41452.09 ns

=== ByteLevel Pre-tokenization Statistics ===
Total calls: 3000
Average per call: 15103.57 ns

Phase breakdown (average per call):
  1. Prefix space handling:       22.56 ns (  0.1%)
  2. Regex splitting:           8003.47 ns ( 53.0%)
  3. Byte-level transform:      7077.55 ns ( 46.9%)
============================================

Profiling batch encode...
Batch encode:          7963.02 ns (Total: 7 ms)
