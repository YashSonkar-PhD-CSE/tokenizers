Loading tokenizer from: data/tokenizer.json
Reading text from: data/brown_corpus.txt
Profiling 1000 lines...
--- Profiling Results (Average per line) ---
Normalization:         1403.45 ns
Pre-tokenization:     13969.53 ns
Model Tokenization:    3772.70 ns
Post-processing:        622.47 ns
Total (Pipeline):     19768.15 ns
Total (End-to-End):   38703.93 ns

=== ByteLevel Pre-tokenization Statistics ===
Total calls: 3000
Average per call: 14417.66 ns

Phase breakdown (average per call):
  1. Prefix space handling:       22.50 ns (  0.2%)
  2. Regex splitting:           7757.06 ns ( 53.8%)
  3. Byte-level transform:      6638.09 ns ( 46.0%)

Total time breakdown:
  Prefix space handling:          67515 ns
  Regex splitting:             23271190 ns
  Byte-level transform:        19914281 ns
  Total:                       43252986 ns
============================================

Profiling batch encode...
Batch encode:          8488.33 ns (Total: 8 ms)